# Use a CUDA runtime base image (GPU support)
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    git \
    wget \
    unzip \
    libsndfile1 \
    ffmpeg \
    libffi-dev \
    libssl-dev \
    libportaudio2 \
    libportaudiocpp0 \
    portaudio19-dev \
    python3.10 \
    python3.10-dev \
    python3.10-venv \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# COPY backend into the path the script expects
COPY backends/face /app/backends/face
# COPY packaging script
COPY scripts/package_zip.py /app/scripts/package_zip.py

# Create virtual environment
RUN python3 -m venv /app/venv

# Upgrade pip and install build tools
RUN /app/venv/bin/pip install --upgrade pip==23.3.2 setuptools wheel
RUN /app/venv/bin/pip install pyinstaller

# Install backend requirements (point to the requirements inside the copied backend)
RUN /app/venv/bin/pip install -r /app/backends/face/requirements.txt --no-cache-dir

# GPU-specific Python packages (note: ensure these match your CUDA/toolkit version)
RUN /app/venv/bin/pip install torch
RUN /app/venv/bin/pip install onnxruntime-gpu

# Build/package the backend
ARG GITHUB_REF_NAME=dev
RUN /app/venv/bin/python /app/scripts/package_zip.py --backend face --device gpu --os linux --version ${GITHUB_REF_NAME}

# Make the start script executable
RUN chmod +x /app/backends/face/start_face.sh

# Set runtime working dir to the backend and expose port
WORKDIR /app/backends/face
EXPOSE 8081

# Run the backend
CMD ["./start_face.sh"]
